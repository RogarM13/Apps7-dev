Problem 2, Analyzing Data:
Developing the application for the Problem 1 I already found one of the issues with the data for the adNetwork adUmbrella where the last row of the dataset contained a summary of the data. That creates a problem firstly with having a row with values that includes different info and has to be discarded in any analysis but also in the data writing process because it is not of the same format. That means it will not comply with any schema we create for the table.
For the purposes of Problem 2 I analyzed all 4 datasets given. I checked for some of the most common scenarios of corrupt/bad data. That included:
- Missing values (especially important if the column is a primary key or an index)
- Numeric outliers (negative values where there can't be, infinite values, outliers that can't be explained e.g. values from 1-100 but one value is 152431)
- String values that do not seem to be correct (E.g. we have a string filed that we know can only be 20 characters long but we have some values that far exceed that or we have a fixed set of possible string values)
- Date data (date info that does not seem to be correct, e.g. 1970-01-01 or 2199-01-01)
The first conclusion was the issue I already found during development for Problem 1. The other problem with the data was for the datasets for adNetwork: SuperNetwork, date: 2022-09-16. There seem to be rows of data where the number of Impressions is larger than the number of Requests. Which does not seem to be possible. Other than that the only additional note I have is that the data for the tuples Date, App, Platform are duplicated in all 4 datasets. It seems like this is just the way the data is designed, t.i. not compressed into 1 row for revenue, number of requests and impressions per Date-App-Platform tuple.
In general the sanity checks depend on the data we have. The most important part is to always check that the data types are compliant (all values are of the same type within a column) and that there are no missing values in index columns. Also important is looking for outliers (weird numeric values, dates,...) and duplicates. More detailed data sanity checks require a more detailed knowledge of the data and what it represents.As a note, one of the data sanity checks that I did not include in this case (since there was not that much data) could be: Manually inspecting a random sample of the data (e.g. looking for differences between the initial 1000 rows compared to a random sample.)

Problem 3, Reliable Design
In the case of such a system there needs to be an orchestration tool to manage the services that would be developed for data collection. Before we decide on how to orchestrate, the first decision would be on how to design the services such as the one developed in Problem 1. It depends on how similar or how different the data collection would be depending on the adNetworks. The solution can be packaged as one service for all adNetworks, where asNetwork is only one of the input parameters. However, if the process logic can't be reproduced to all or even to some groups of adNetworks multiple services would need to be developed. There is also the question of storage and what database or database server to use. And where the solution would live (on prem, cloud).
One of the possibilities is to use Kubernetes and develop the services so that we can use them as always-running pods with monitoring that can be then triggered to execute data collection. The trigger can be scheduled or based on some other type of event. Alternatively a system like Apache Airflow for workflows can be used. This can be done in combination with kubernetes (Apache Airflow as a service on kubernetes) or as a standalone system. There are also other similar workflow systems such as Prefect and Argo. In this case the services to collect the data can be package as DAGs.
The common theme here is to have monitoring of what has happened when the workflow was executed and to have the possibility to execute it again and reproduce runs if necessary.
In Google Cloud, based on the above description and the specifics on how to collect data I see two potential options.
1. The services to collect the data would be developed with GCP Cloud Functions with the use of Secret Manager for sensitive data. For the orchestration tool I would use the GCP native tools Cloud Workflows.
2. The services to collect the data would be implemented as DAGs with Cloud Composer.
In both cases the database storage would be Google's BigQuery (if expecting a lot of data).
